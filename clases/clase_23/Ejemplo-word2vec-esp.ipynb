{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec y redes neuronales para modelos de lenguaje\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de lenguaje\n",
    "\n",
    "\n",
    "(Referencia: [Bengio et al](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf))\n",
    "\n",
    "\n",
    "Si $w=w_1w_2\\cdots w_N$ es una frase, y las $w$ representan palabras,  un **modelo de lenguaje**\n",
    "es una distribución de probabilidad sobre posibles oraciones\n",
    "\n",
    "$$P(w_1w_2\\cdots w_N).$$\n",
    "\n",
    "Estos modelos son importantes en varias áreas de procesamiento de lenguaje natural. Por ejemplo, son utilizados en reconocimiento de lenguaje hablado, reconocimiento de escritura, corrección de ortografía, etc. \n",
    "\n",
    "Por ejemplo, en corrección de ortografía (ejemplo simple), el modelo de lenguaje es el que nos permite escoger entre varias alternativas de corrección: si escribimos *'el peroo corre'*, tenemos como alternativas *'el pero corre'*, *'el perno corre'* o *'el perro corre'*. Un modelo de lenguaje adecuado señalaría como corrección apropiada *'el perro corre'*, pues es una oración mucho más probable que las otras dos.\n",
    "\n",
    "\n",
    "Muchas veces los modelos de lenguaje se construyen a partir de probabilidades condicionales:\n",
    "\n",
    "$$P(w|w_1\\ldots w_s),$$\n",
    "\n",
    "para cualquier palabra $w$ y frase $w_1\\ldots w_s$. Podemos utilizar entonces la regla del producto para calcular cualquier $P(w_1w_2\\cdots w_N).$ En la práctica, sin embargo, es difícil construir estas probabiilidades para cualquier frase $w_1\\ldots w_s$ (hay una cantidad inmensa de posibles frases a las qué condicionar), y aproximamos utilizando\n",
    "una dependencia relativamente corta. Por ejemplo, el modelo de trigramas está dado por las probabilidades\n",
    "\n",
    "$$P(w_3 | w_1w_2),$$\n",
    "\n",
    "y usamos estas para calcular la probabilidad de una frase que puede ser de longitud arbitraria $N$ usando la regla del producto.\n",
    "\n",
    "Con una colección de textos grandes,  se pueden obtener buenos modelos simplemente contando para estimar estas probabilidades ([Google ngram viewer](https://books.google.com/ngrams)). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales y modelos de lenguaje\n",
    "\n",
    "\n",
    "### Idea básica\n",
    "Estos modelos de lenguaje también pueden construirse con redes neuronales. La idea básica es\n",
    "que la capa de entrada sea por ejemplo el par de palabras $w_1, w_2$, codificadas con vectores indicadores.\n",
    "\n",
    "Por ejemplo, si el lenguaje tiene $V=5$ palabras, que son 'hola','adiós','y','el','día', identificamos cada palabra con un entero 1,2,3,4,5. Si observamos  el par de palabras 'hola y', codificaríamos (codificación dummy o indicadoras) 'hola' a (1,0,0,0,0) y 'y' a (0,0,1,0,0), y la capa de entrada (con 10 variables) sería entonces\n",
    "$$z=((1,0,0,0,0),(0,0,1,0,0)).$$\n",
    "\n",
    "Después de esta capa pondremos algunas capas ocultas, y finalmente tenemos la capa de salida\n",
    "es un vector de tamaño $V=5$.\n",
    "$$(p_1,\\ldots,p_5)$$\n",
    "que nos da la probabilidad de que cada posible palabra ocurra después de 'hola y'.\n",
    "\n",
    "\n",
    "### Arquitectura\n",
    "\n",
    "La primera idea importante para estas redes está en la arquitectura de la primera capa, que está definida por una matrix $C$ de tamaño $m\\times V$. \n",
    "\n",
    "Obsérvese que si $e_i$ es un vector indicador ($i$-ésima componente igual a 1 y el resto 0), $Ce$ es simplemente la columna $i$ de la matriz $C$. **En la primera capa, usamos la matriz de pesos $C$ para todas las palabras independientemente de su posición**.\n",
    "\n",
    "\n",
    "En nuestro ejemplo, si $C$ de tamaño $m\\times V=m\\times 5$, haríamos\n",
    "\n",
    "$$z=((1,0,0,0,0),(0,0,1,0,0)) = (e_1,e_3) = (Ce_1, Ce_3) = (x_1,x_2) = x,$$\n",
    "\n",
    "donde nótese que usamos la misma matriz $C$ para ambos vectores. Tenemos entonces pesos compartidos de $C$ sobre las palabras de entrada. \n",
    "\n",
    "En esta primera capa, que sólo extrae los coeficientes apropiados, no hace falta agregar una función no-lineal (esta capa simplemente asigna un vector a cada palabra de entrada).\n",
    "\n",
    "\n",
    "En el modelo de Bengio, seguimos con una capa oculta totalmente conexa y la capa de salida totalmente conexa:\n",
    "\n",
    "La siguiente capa oculta es entonces\n",
    "\n",
    "$$w = h( b+Hx),$$\n",
    "\n",
    "donde $h$ es la función logística o tangente hiperbólica, aplicada componente a componente, y finalmente la capa de salida dada por\n",
    "\n",
    "$$y = a+Uw$$\n",
    "seguido de softmax\n",
    "$$p_i = \\frac{e^{y_i}}{\\sum_j e^{y_j}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Resumen: modelo de red neuronal\n",
    "\n",
    "Podemos entonces construir una red neuronal con 2 capas ocultas como sigue: Usemos el ejemplo de trigramas:\n",
    "\n",
    "1. En la primera capa oculta, tenemos un mapeo de las entradas $w_1,\\ldots, w_{n-1}$ a $x=C(w_1),\\ldots, C(w_{n-1})$, donde $C$ es una función que mapea palabras a vectores de dimensión $d$. $C$ también se puede pensar como una matriz de dimensión $|V|$ por $d$. En la capa de entrada,\n",
    "\n",
    "$$w_{n-2},w_{n-1} \\to x = (C(w_{n-2}), C(w_{n-1})).$$\n",
    "\n",
    "que es un vector de tamaño $2d$.\n",
    "\n",
    "\n",
    "2. En la siguiente capa oculta tenemos una matriz de pesos $H$ y la función logística (o tangente hiperbólica) $\\sigma$, como en una red neuronal usual. En esta capa calculamos\n",
    "$$z = h (a + Hx),$$\n",
    "que resulta en un vector de tamaño $h$. \n",
    "\n",
    "3. Finalmente, la capa de salida debe ser un vector de probabilidades\n",
    "sobre todo el vocabulario $|V|$. En esta capa tenemos pesos $U$ y hacemos\n",
    "$$y = b + Uh (z),$$\n",
    "y finalmente usamos softmax para tener probabilidades que suman uno:\n",
    "$$p_i = \\frac{\\exp (y_i) }{\\sum_j exp(y_j)}.$$\n",
    "\n",
    "En el ajuste maximizamos la verosimilitud:\n",
    "\n",
    "$$\\sum_t \\log \\hat{P}(w_{t,n}|w_{t,n-2}w_{t-n-1})$$\n",
    "\n",
    "\n",
    "La representación en la referencia (Bagio) es:\n",
    "\n",
    "![Imagen](1_neural_model.png)\n",
    "\n",
    "Esta idea original ha sido explotada con éxito, aunque sigue siendo\n",
    "muy intensivo en cómputo ajustar un modelo como este. Nótese que\n",
    "el número de parámetros es del orden de $|V|(m+h)$, donde $|V|$ es el tamaño del vocabulario (decenas o cientos de miles),  $m$ es el tamaño de la representacion (cientos o miles) y $h$ es el número de nodos en la segunda capa (también cientos o miles).  Esto resulta en el mejor de los casos en modelos con miles de millones de parámetros. Adicionalmente, hay algunos cálculos costosos, como el softmax (donde hay que hacer una suma sobre el vocabulario completo). En el paper original se propone gradiente estocástico paralelizado.\n",
    "\n",
    "\n",
    "### Representación de palabras\n",
    "\n",
    "Un aspecto interesante de el modelo de arriba es que\n",
    "nos da una representación vectorial de las palabras, en la forma\n",
    "de los parámetros ajustados de la matriz $C$. Esta se puede entender\n",
    "como una descripción numérica de cómo funciona una palabra en el contexto de su n-grama.\n",
    "\n",
    "Por ejemplo, deberíamos encontrar que palabras como \"perro\" y \"gato\" tienen representaciones similares. La razón es que cuando aparecen,\n",
    "las probabilidades sobre las palabras siguientes deberían ser similares, pues estas son dos palabras que se pueden usar en muchos contextos\n",
    "compartidos.\n",
    "\n",
    "También podríamos encontrar que palabras como perro, gato, águila, león, etc. tienen partes o entradas similares en sus vectores de representación, que es la parte que hace que funcionen como \"animal mamífero\" dentro de frases. \n",
    "\n",
    "Veremos que hay más razones por las que es interesante esta representación.\n",
    "\n",
    "\n",
    "## Modelos de word2vec de Google\n",
    "\n",
    "Referencias: [word2vec](http://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "Si lo que principalmente nos interesa es obtener la representación\n",
    "vectorial de palabras, recientemente se descubrió que es posible \n",
    "simplificar considerablemente el modelo de arriba para poder entrenarlo mucho más rápido, y obtener una representación que en muchas tareas se desempeña muy bien.\n",
    "\n",
    "Hay dos ideas básicas para reducir la complejidad del entrenamiento:\n",
    "\n",
    "- Eliminar la segunda capa oculta.\n",
    "- Cambiar la función objetivo (minimizar devianza/maximizar verosimilitud) por una más simple, mediante un truco que se llama \"negative sampling\".\n",
    "\n",
    "Se usa un enfoque un poco distinto (no es necesario predecir la siguiente palabra) :  **intentamos predecir la palabra\n",
    "central a partir de las que están alrededor**. Esto es porque para los problemas que se ha aplicado esta resulta una mejor estrategia (problema de analogías, por ejemplo, que veremos más adelante).\n",
    "\n",
    "### Arquitectura continuous bag-of-words\n",
    "\n",
    "La entrada es igual que en el modelo completo. En primer lugar,\n",
    "simplificamos la segunda capa oculta pondiendo en $z$ el promedio de\n",
    "los vectores $C(w_{n+1}), C(w_{n-1})$ (que rodean a $w_n$).  La última capa la dejamos igual, y por el momento:\n",
    "\n",
    "![Imagen](cbow_fig.png)\n",
    "\n",
    "\n",
    "\n",
    "El modelo se llama bag-of-words porque todas las entradas de la primera capa oculta contribuyen de la misma manera en la salida, independientemente del orden. Aunque esto no suena como buena idea para construir un modelo de lenguaje, veremos que resulta en una representación adecuada para algunos problemas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. En la primera capa oculta, tenemos un mapeo de las entradas $w_1,\\ldots, w_{n-1}$ a $x=(C(w_1),\\ldots, C(w_{n-1}))$, donde $C$ es una función que mapea palabras a vectores de dimensión $d$. $C$ también se puede pensar como una matriz de dimensión $|V|$ por $d$. En la capa de entrada,\n",
    "\n",
    "$$w_{n+1},w_{n-1} \\to x = (C(w_{n+1}), C(w_{n-1})).$$\n",
    "\n",
    "\n",
    "2. En la siguiente \"capa\"\" oculta simplemente sumamos las entradas de $x$ para obtener $z$, que es también un vector de longitud $d$. Aquí nótese que realmente no hay parámetros, solo una proyección.\n",
    "\n",
    "3. Finalmente, la capa de salida debe ser un vector de probabilidades\n",
    "sobre todo el vocabulario $|V|$. En esta capa tenemos pesos $U$ y hacemos ($h$ es la función logística):\n",
    "$$y = b + Uh (z),$$\n",
    "y finalmente usamos softmax para tener probabilidades que suman uno:\n",
    "$$p_i = \\frac{\\exp (y_i) }{\\sum_j exp(y_j)}.$$\n",
    "\n",
    "En el ajuste maximizamos la verosimilitud:\n",
    "\n",
    "$$\\sum_t \\log \\hat{P}(w_{t,n}|w_{t,n+1} \\cdots w_{t-n-1})$$\n",
    "\n",
    "Todavía se propone una simplificación adicional\n",
    "\n",
    "### Muestreo negativo\n",
    "\n",
    "La siguiente simplificación consiste en cambiar la función objetivo. En word2vec puede usarse \"muestreo negativo\".\n",
    "\n",
    "Para empezar, la función objetivo original (para un solo trigrama) es\n",
    "\n",
    "\n",
    "$$E` = -\\log \\hat{P}(w_n|w_{n-1}w_{n+1}) = -y_w + \\log\\sum_j \\exp(y_j).$$\n",
    "\n",
    "La dificultad está en el segundo término, que es sobre todo el vocabulario en incluye todos los parámetros del modelo.\n",
    "\n",
    "\n",
    "La idea del muestreo negativo es que si observamos\n",
    "$w_n$ entre de $w_{n+1}$ y $w_{n-1}$, tomamos una muestra de $k$ palabras\n",
    "$v_1,\\ldots v_k$ al azar\n",
    "(2-100, dependiendo del tamaño de la colección), y creamos $k$\n",
    "\"trigramas falsos\" $w_{n-2} v_j w_{n-1}$, $j=1\\ldots,k$. Minimizamos\n",
    "en lugar de la observación de arriba\n",
    "\n",
    "$$E = -\\log h(y_w) - \\sum_k \\log h (-y_k).$$\n",
    "\n",
    "Es decir, solo buscamos optimizar parámetros para separar lo mejor\n",
    "que podamos la observación \"verdadera\" de las $k$ observaciones \"falsas\".\n",
    "\n",
    "\n",
    "\n",
    "### Arquitectura continuous skip-gram\n",
    "\n",
    "\n",
    "Es también posibles (y en algunos casos con mejores resultados) usar la siguiente estructura, donde intentamos predecir el contexto a partir de cada palabra:\n",
    "\n",
    "![Imagen](skip-gram.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Consideramos una colección de fragmentos de noticias de periódicos españoles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_esp = pd.read_csv(\"datos/Es_Newspapers.txt\",\n",
    "                        delimiter=\"\\t\", header=None, quoting=3, encoding = 'utf-8')\n",
    "\n",
    "train_esp.columns = ['fragmento']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fragmento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En este sentido, señala que «no podemos consen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Cuando acabe la experiencia con el Inter no m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Teniendo salud se aguanta todo», dice Nati Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'72 días': De Danilo &amp;ScaronerbedÅ¾ija. Croaci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Administración norteamericana no ha dado aú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tras ensalzar la figura de Arenas como ex mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El doctor del área de Prehistoria de la UMU, L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>La diputada provincial de Servicios Sociales, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>En este punto, la corte acepta las explicacion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>En Santander, la cesión de los terrenos en man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>El Gobierno de Canarias le ha concedido la Med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Un australiano de 65 años fue acusado de condu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>El informe del CVC, que se ha emitido a petici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>encuestas de coyuntura. Esta opinión es confir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>El público volvió a abarrotar el recinto unive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>La detención de presunto autor de los hechos, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>En caso contrario se reconocería solo la condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>El sindicato anuncia que, en unidad con UGT, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SANTIAGO MARTÍNEZ LAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Curándose en salud, el Gobierno decidió incorp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>«Está claro que lo que están esperando ya es q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>No es habitual que una Administración pública ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Times asegura que el evento más caro será ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>El Departamento de Estado organizó el traslado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Un aparejador que participó en las obras de re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>La Diputación Foral de Gipuzkoa ha aprobado la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>La delegación tiene también como objetivo cono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>En la misma línea que Quinzá, el también popul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jorge Lorenzo (Yamaha) llega a California como...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Robin Soderling, quinto favorito del Abierto d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309888</th>\n",
       "      <td>El presidente afgano, Hamid Karzai, ha obtenid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309889</th>\n",
       "      <td>Una vez pasado ese 'corte' inicial al pisar lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309890</th>\n",
       "      <td>Por su parte, la red nocturna prestará servici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309891</th>\n",
       "      <td>Es por ello, que esta noche de verano será una...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309892</th>\n",
       "      <td>Montalván añadió que «las sesiones de trabajo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309893</th>\n",
       "      <td>Los nuevos presupuestos para el 2009 ya han si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309894</th>\n",
       "      <td>¿Cuál es la mala?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309895</th>\n",
       "      <td>En el concepto de los parques se ha considerad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309896</th>\n",
       "      <td>El proyecto se abordará en dos fases, la prime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309897</th>\n",
       "      <td>Allí, comenzaron a liberar a todas las mujeres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309898</th>\n",
       "      <td>\"Pudimos hacer más, sobre todo en defensa\". Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309899</th>\n",
       "      <td>En el centro de votación más importante de Mia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309900</th>\n",
       "      <td>De esta forma, el equipo infantil y el cadete ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309901</th>\n",
       "      <td>Hay un dicho popular en la industria aeronáuti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309902</th>\n",
       "      <td>Como principales novedades, Sotoca subrayó el ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309903</th>\n",
       "      <td>Bajo una intensa lluvia, que estuvo presente d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309904</th>\n",
       "      <td>La Denominación de Origen Rueda cumplió los 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309905</th>\n",
       "      <td>El fichaje de Martin, que llegó a Ferrari tras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309906</th>\n",
       "      <td>Dos personas han fallecido y otras tres han re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309907</th>\n",
       "      <td>Por su parte, el secretario de la Federación d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309908</th>\n",
       "      <td>-Generalmente sí. En Valencia me pasé 48 horas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309909</th>\n",
       "      <td>N o encabezan la lista de las más ricas de 'Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309910</th>\n",
       "      <td>Una de las explicaciones reside, además de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309911</th>\n",
       "      <td>La consejera, según han informado fuentes del ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309912</th>\n",
       "      <td>Y ese propio camino se andaría en Irán en plen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309913</th>\n",
       "      <td>Pero la Ley de Enjuiciamiento Civil autoriza l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309914</th>\n",
       "      <td>¿Qué tal si salieran a sorteo público, como lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309915</th>\n",
       "      <td>«Queremos que los ciudadanos sean los vigilant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309916</th>\n",
       "      <td>Berodia es duda y descartado está Tomás, que a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309917</th>\n",
       "      <td>El Reino Unido cuenta con 9.500 efectivos en A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309918 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fragmento\n",
       "0       En este sentido, señala que «no podemos consen...\n",
       "1       \"Cuando acabe la experiencia con el Inter no m...\n",
       "2       «Teniendo salud se aguanta todo», dice Nati Mi...\n",
       "3       '72 días': De Danilo &ScaronerbedÅ¾ija. Croaci...\n",
       "4       La Administración norteamericana no ha dado aú...\n",
       "5       Tras ensalzar la figura de Arenas como ex mini...\n",
       "6       El doctor del área de Prehistoria de la UMU, L...\n",
       "7       La diputada provincial de Servicios Sociales, ...\n",
       "8       En este punto, la corte acepta las explicacion...\n",
       "9       En Santander, la cesión de los terrenos en man...\n",
       "10      El Gobierno de Canarias le ha concedido la Med...\n",
       "11      Un australiano de 65 años fue acusado de condu...\n",
       "12      El informe del CVC, que se ha emitido a petici...\n",
       "13      encuestas de coyuntura. Esta opinión es confir...\n",
       "14      El público volvió a abarrotar el recinto unive...\n",
       "15      La detención de presunto autor de los hechos, ...\n",
       "16      En caso contrario se reconocería solo la condi...\n",
       "17      El sindicato anuncia que, en unidad con UGT, a...\n",
       "18                                 SANTIAGO MARTÍNEZ LAGE\n",
       "19      Curándose en salud, el Gobierno decidió incorp...\n",
       "20      «Está claro que lo que están esperando ya es q...\n",
       "21      No es habitual que una Administración pública ...\n",
       "22      The Times asegura que el evento más caro será ...\n",
       "23      El Departamento de Estado organizó el traslado...\n",
       "24      Un aparejador que participó en las obras de re...\n",
       "25      La Diputación Foral de Gipuzkoa ha aprobado la...\n",
       "26      La delegación tiene también como objetivo cono...\n",
       "27      En la misma línea que Quinzá, el también popul...\n",
       "28      Jorge Lorenzo (Yamaha) llega a California como...\n",
       "29      Robin Soderling, quinto favorito del Abierto d...\n",
       "...                                                   ...\n",
       "309888  El presidente afgano, Hamid Karzai, ha obtenid...\n",
       "309889  Una vez pasado ese 'corte' inicial al pisar lo...\n",
       "309890  Por su parte, la red nocturna prestará servici...\n",
       "309891  Es por ello, que esta noche de verano será una...\n",
       "309892  Montalván añadió que «las sesiones de trabajo ...\n",
       "309893  Los nuevos presupuestos para el 2009 ya han si...\n",
       "309894                                  ¿Cuál es la mala?\n",
       "309895  En el concepto de los parques se ha considerad...\n",
       "309896  El proyecto se abordará en dos fases, la prime...\n",
       "309897  Allí, comenzaron a liberar a todas las mujeres...\n",
       "309898  \"Pudimos hacer más, sobre todo en defensa\". Ba...\n",
       "309899  En el centro de votación más importante de Mia...\n",
       "309900  De esta forma, el equipo infantil y el cadete ...\n",
       "309901  Hay un dicho popular en la industria aeronáuti...\n",
       "309902  Como principales novedades, Sotoca subrayó el ...\n",
       "309903  Bajo una intensa lluvia, que estuvo presente d...\n",
       "309904  La Denominación de Origen Rueda cumplió los 30...\n",
       "309905  El fichaje de Martin, que llegó a Ferrari tras...\n",
       "309906  Dos personas han fallecido y otras tres han re...\n",
       "309907  Por su parte, el secretario de la Federación d...\n",
       "309908  -Generalmente sí. En Valencia me pasé 48 horas...\n",
       "309909  N o encabezan la lista de las más ricas de 'Fo...\n",
       "309910  Una de las explicaciones reside, además de la ...\n",
       "309911  La consejera, según han informado fuentes del ...\n",
       "309912  Y ese propio camino se andaría en Irán en plen...\n",
       "309913  Pero la Ley de Enjuiciamiento Civil autoriza l...\n",
       "309914  ¿Qué tal si salieran a sorteo público, como lo...\n",
       "309915  «Queremos que los ciudadanos sean los vigilant...\n",
       "309916  Berodia es duda y descartado está Tomás, que a...\n",
       "309917  El Reino Unido cuenta con 9.500 efectivos en A...\n",
       "\n",
       "[309918 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es por ello, que esta noche de verano será una de las más alternativas y originales de las que se ofrecen en la Feria de Julio en Valencia.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print train_esp[\"fragmento\"][309891]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaremos una función para separar oraciones en palabras (eliminamos signos de puntuación, números). También convertimos por facilidad todo a minúsculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "por\n",
      "ello\n",
      "que\n",
      "esta\n",
      "noche\n",
      "de\n",
      "verano\n",
      "será\n",
      "una\n",
      "de\n",
      "las\n",
      "más\n",
      "alternativas\n",
      "y\n",
      "originales\n",
      "de\n",
      "las\n",
      "que\n",
      "se\n",
      "ofrecen\n",
      "en\n",
      "la\n",
      "feria\n",
      "de\n",
      "julio\n",
      "en\n",
      "valencia\n"
     ]
    }
   ],
   "source": [
    "def texto_palabras(oracion):  \n",
    "    #dejar solo caracteres alfanuméricos\n",
    "    oracion_texto = re.sub(u\"[^a-zA-Záéíóúñ]\",\" \", oracion)\n",
    "    #separar las palabras después de ponerlas en minúsculas\n",
    "    palabras = oracion_texto.lower().split()\n",
    "    return(palabras)    \n",
    "palabras = texto_palabras( train_esp[\"fragmento\"][309891])\n",
    "for p in palabras:\n",
    "    print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dividir los textos en oraciones, y necesitamos que cada una oración esté representada como una lista de palabras. Para eso usamos la siguiente función:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "\n",
    "def texto_oraciones( texto, tokenizer ):\n",
    "    oraciones_lista = tokenizer.tokenize(texto.strip())\n",
    "    oraciones = []\n",
    "    for oracion in oraciones_lista:        \n",
    "        if len(oracion) > 0:\n",
    "            oraciones.append( texto_palabras( oracion ))\n",
    "    return oraciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera oración\n",
      "[u'en', u'este', u'sentido', u'se\\xf1ala', u'que', u'no', u'podemos', u'consentir', u'que', u'se', u'repita', u'el', u'malogrado', u'caso', u'del', u'centro', u'de', u'transportes', u'de', u'benavente', u'donde', u'la', u'falta', u'de', u'control', u'ha', u'supuesto', u'un', u'c\\xfamulo', u'de', u'irregularidades', u'que', u'rozan', u'lo', u'delictivo']\n",
      "Segunda oración\n",
      "[u'cuando', u'acabe', u'la', u'experiencia', u'con', u'el', u'inter', u'no', u'me', u'quedar\\xe9', u'en', u'italia', u'sino', u'que', u'espero', u'ir', u'a', u'espa\\xf1a', u'porque', u'mi', u'objetivo', u'es', u'ganar', u'los', u't\\xedtulos', u'de', u'los', u'tres', u'campeonatos', u'm\\xe1s', u'competitivos', u'del', u'mundo', u'afirm\\xf3', u'el', u'que', u'fuera', u'entrenador', u'del', u'barcelona', u'b', u'y', u'a\\xf1adi\\xf3', u'me', u'falta', u'liga']\n"
     ]
    }
   ],
   "source": [
    "oraciones = []\n",
    "for texto in train_esp[\"fragmento\"][:3]:\n",
    "    oraciones += texto_oraciones(texto, tokenizer)\n",
    "print \"Primera oración\"\n",
    "print oraciones[0]\n",
    "print \"Segunda oración\"\n",
    "print oraciones[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718544"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones = []\n",
    "for texto in train_esp[\"fragmento\"]:\n",
    "    oraciones += texto_oraciones(texto, tokenizer)\n",
    "\n",
    "print len(oraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Prueba de logging.\n"
     ]
    }
   ],
   "source": [
    "## Este código sólo es necesario si queremos ver las salidas durante el\n",
    "## proceso de ajuste\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "log = logging.getLogger()\n",
    "f = logging.Formatter(\"%(asctime)s - %(module)s.   %(funcName)s - %(levelname)s - %(message)s\")\n",
    "fh = logging.handlers.TimedRotatingFileHandler('log.txt', 'W6')\n",
    "fh.setFormatter(f)\n",
    "log.addHandler(fh)\n",
    "log.setLevel(logging.INFO)\n",
    "log.info('Prueba de logging.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Valores para parámetros\n",
    "num_features = 150    # Dimensión de los vectores de palabras\n",
    "min_word_count = 40   # Mínima frecuencia para incluir en el modelo                        \n",
    "num_workers = 6       # Número de procesos\n",
    "context = 6         # El contexto es el tamaño de n-gramas (10 es 5 al principio y 5 al final)                                                                                    \n",
    "downsampling = 0  # Los autores recomiendan submuestrar palabras muy frecuentes (0.0001 y 0.01)\n",
    "k_negative = 30       # Cuántas muestras negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 260339 words, keeping 27849 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 520837 words, keeping 40519 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 780387 words, keeping 49659 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 1037751 words, keeping 57292 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 1300719 words, keeping 63935 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 1560025 words, keeping 69439 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 1817263 words, keeping 74735 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 2077525 words, keeping 79703 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 2336067 words, keeping 83928 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 2596624 words, keeping 88044 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 2856400 words, keeping 91849 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 3118220 words, keeping 95529 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 3375572 words, keeping 98902 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 3634053 words, keeping 102149 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 3893417 words, keeping 105239 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 4157053 words, keeping 108365 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #170000, processed 4416130 words, keeping 111264 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #180000, processed 4677288 words, keeping 114148 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #190000, processed 4936575 words, keeping 116777 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #200000, processed 5197470 words, keeping 119205 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #210000, processed 5459016 words, keeping 121819 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #220000, processed 5719448 words, keeping 124223 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #230000, processed 5981652 words, keeping 126607 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #240000, processed 6244024 words, keeping 128990 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #250000, processed 6506322 words, keeping 131347 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #260000, processed 6767210 words, keeping 133551 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #270000, processed 7027505 words, keeping 135845 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #280000, processed 7287186 words, keeping 137979 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #290000, processed 7547362 words, keeping 139948 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #300000, processed 7806464 words, keeping 141889 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #310000, processed 8066570 words, keeping 143881 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #320000, processed 8324212 words, keeping 145802 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #330000, processed 8584279 words, keeping 147735 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #340000, processed 8844675 words, keeping 149539 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #350000, processed 9103821 words, keeping 151307 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #360000, processed 9363626 words, keeping 153074 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #370000, processed 9624731 words, keeping 154809 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #380000, processed 9879924 words, keeping 156446 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #390000, processed 10140346 words, keeping 158116 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #400000, processed 10400781 words, keeping 159806 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #410000, processed 10661890 words, keeping 161383 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #420000, processed 10924229 words, keeping 162880 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #430000, processed 11187643 words, keeping 164593 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #440000, processed 11446433 words, keeping 166144 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #450000, processed 11706544 words, keeping 167688 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #460000, processed 11965803 words, keeping 169161 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #470000, processed 12224684 words, keeping 170680 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #480000, processed 12485458 words, keeping 172141 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #490000, processed 12743772 words, keeping 173620 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #500000, processed 13005619 words, keeping 175028 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #510000, processed 13261145 words, keeping 176453 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #520000, processed 13522626 words, keeping 177783 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #530000, processed 13783498 words, keeping 179220 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #540000, processed 14049146 words, keeping 180688 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #550000, processed 14311791 words, keeping 182067 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #560000, processed 14572914 words, keeping 183424 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #570000, processed 14830804 words, keeping 184756 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #580000, processed 15092063 words, keeping 185990 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #590000, processed 15351568 words, keeping 187318 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #600000, processed 15612049 words, keeping 188622 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #610000, processed 15872940 words, keeping 189875 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #620000, processed 16132396 words, keeping 191123 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #630000, processed 16394849 words, keeping 192383 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #640000, processed 16655918 words, keeping 193507 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #650000, processed 16916081 words, keeping 194754 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #660000, processed 17175859 words, keeping 195937 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #670000, processed 17437903 words, keeping 197186 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #680000, processed 17699933 words, keeping 198407 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #690000, processed 17956915 words, keeping 199613 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #700000, processed 18219678 words, keeping 200744 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #710000, processed 18480832 words, keeping 201961 word types\n",
      "INFO:gensim.models.word2vec:collected 202955 word types from a corpus of 18705407 raw words and 718544 sentences\n",
      "INFO:gensim.models.word2vec:min_count=40 retains 22192 unique words (drops 180763)\n",
      "INFO:gensim.models.word2vec:min_count leaves 17801680 word corpus (95% of original 18705407)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 202955 items\n",
      "INFO:gensim.models.word2vec:sample=0 downsamples 0 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 17801680 word corpus (100.0% of prior 17801680)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 22192 words and 150 dimensions: 37726400 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.word2vec:training model with 6 workers on 22192 vocabulary and 150 features, using sg=1 hs=0 sample=0 and negative=30\n",
      "INFO:gensim.models.word2vec:expecting 718544 examples, matching count from corpus used for vocabulary survey\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.64% examples, 114450 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.29% examples, 111229 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.96% examples, 113851 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.62% examples, 113864 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.24% examples, 113616 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.92% examples, 114619 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.59% examples, 114441 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.20% examples, 112895 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.83% examples, 112616 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.47% examples, 112924 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.07% examples, 112132 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.70% examples, 112128 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.35% examples, 112382 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.95% examples, 111733 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.57% examples, 111505 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.22% examples, 111486 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.81% examples, 110994 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.47% examples, 111106 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.08% examples, 110877 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.72% examples, 110849 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.33% examples, 110719 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.96% examples, 110823 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.59% examples, 110519 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.23% examples, 110619 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.84% examples, 110447 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.46% examples, 110509 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.08% examples, 110223 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.72% examples, 110327 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.31% examples, 110088 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.94% examples, 110087 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.53% examples, 109836 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.05% examples, 109232 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.60% examples, 108723 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.15% examples, 108404 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.68% examples, 107981 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.27% examples, 107848 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.87% examples, 107724 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.49% examples, 107801 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.10% examples, 107727 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.72% examples, 107708 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.30% examples, 107588 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.93% examples, 107632 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.53% examples, 107476 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.15% examples, 107466 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.71% examples, 107331 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.27% examples, 107162 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.78% examples, 106809 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.36% examples, 106651 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.98% examples, 106597 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.59% examples, 106640 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.17% examples, 106590 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.77% examples, 106591 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.36% examples, 106559 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.94% examples, 106529 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.53% examples, 106424 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.10% examples, 106341 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.68% examples, 106272 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.25% examples, 106155 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.82% examples, 106026 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.41% examples, 105975 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.02% examples, 105981 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.60% examples, 105855 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.19% examples, 105822 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.75% examples, 105684 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.32% examples, 105613 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.84% examples, 105412 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.37% examples, 105263 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.90% examples, 105093 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.49% examples, 105039 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.04% examples, 104925 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.57% examples, 104747 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.11% examples, 104597 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.64% examples, 104386 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.14% examples, 104163 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.65% examples, 103944 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.15% examples, 103742 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.72% examples, 103682 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.22% examples, 103496 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.75% examples, 103312 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.26% examples, 103142 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.78% examples, 102994 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.32% examples, 102887 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.89% examples, 102854 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.43% examples, 102768 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.96% examples, 102636 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.48% examples, 102526 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.05% examples, 102527 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.63% examples, 102498 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.17% examples, 102403 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.69% examples, 102261 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.23% examples, 102169 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.78% examples, 102105 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.37% examples, 102117 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.96% examples, 102147 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.57% examples, 102153 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.14% examples, 102157 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.75% examples, 102221 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.32% examples, 102197 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.88% examples, 102168 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.47% examples, 102178 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.06% examples, 102218 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.61% examples, 102129 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.14% examples, 102020 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.69% examples, 101947 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.23% examples, 101890 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.79% examples, 101860 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.32% examples, 101777 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.88% examples, 101712 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.38% examples, 101581 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.88% examples, 101457 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.41% examples, 101400 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.02% examples, 101418 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.63% examples, 101427 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.22% examples, 101422 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.82% examples, 101449 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.39% examples, 101440 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.98% examples, 101472 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.56% examples, 101454 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.11% examples, 101415 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.68% examples, 101386 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.28% examples, 101396 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.82% examples, 101313 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.41% examples, 101342 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.99% examples, 101358 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.58% examples, 101339 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.15% examples, 101336 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.69% examples, 101308 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.19% examples, 101196 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.69% examples, 101113 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.11% examples, 100906 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.43% examples, 100572 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.83% examples, 100351 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.33% examples, 100260 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.91% examples, 100239 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.48% examples, 100234 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.06% examples, 100248 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.65% examples, 100273 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.24% examples, 100290 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.81% examples, 100296 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.41% examples, 100324 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.01% examples, 100356 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.60% examples, 100368 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.21% examples, 100410 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.75% examples, 100379 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.34% examples, 100400 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.91% examples, 100404 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.49% examples, 100433 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.01% examples, 100353 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.51% examples, 100240 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.01% examples, 100141 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.54% examples, 100088 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.08% examples, 100072 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.64% examples, 100037 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.22% examples, 100043 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.82% examples, 100062 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.39% examples, 100059 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.96% examples, 100063 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.47% examples, 100001 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.03% examples, 100001 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.60% examples, 99990 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.19% examples, 99987 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.73% examples, 99971 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.29% examples, 99968 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.87% examples, 99986 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.44% examples, 99962 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.04% examples, 99982 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.57% examples, 99935 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.18% examples, 99936 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.77% examples, 99950 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.31% examples, 99927 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.88% examples, 99937 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.44% examples, 99931 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.98% examples, 99910 words/s\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.52% examples, 99885 words/s\n",
      "INFO:gensim.models.word2vec:reached end of input; waiting to finish 18 outstanding jobs\n",
      "INFO:gensim.models.word2vec:training on 18705407 raw words took 178.2s, 99876 trained words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(oraciones, workers=num_workers, hs=0,\n",
    "                          size=num_features, min_count = min_word_count,\n",
    "                          window = context, sample = downsampling, negative = k_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15794076, -0.15631652, -0.36642364,  0.04203986, -0.11601214,\n",
       "        0.25709817, -0.4137066 , -0.34088609, -0.30878621,  0.11356426,\n",
       "       -0.35285285, -0.33517739, -0.12443205,  0.04128909, -0.10767747,\n",
       "        0.48707077, -0.21437369,  0.30905855,  0.06170418,  0.21863939,\n",
       "        0.06672436,  0.03327598, -0.26200852, -0.142635  , -0.31035939,\n",
       "        0.12735093, -0.15826052, -0.05973155,  0.05695412, -0.10330881,\n",
       "       -0.09728532, -0.03760261, -0.328592  , -0.17433362, -0.58762729,\n",
       "        0.47216046, -0.37357807,  0.34607968, -0.71195924, -0.15372393,\n",
       "       -0.30316225,  0.13135159, -0.06422084, -0.02370847, -0.12462158,\n",
       "        0.06934674,  0.04028832, -0.02416475, -0.15595137, -0.04859674,\n",
       "        0.21444683, -0.05047542, -0.06709774, -0.12388422,  0.09956151,\n",
       "        0.78626192, -0.37894425, -0.29213643, -0.37914097, -0.01673727,\n",
       "        0.46280301, -0.02709499,  0.27914631, -0.3597815 ,  0.25356546,\n",
       "       -0.28640878, -0.22427273, -0.23685978, -0.23272741, -0.47627386,\n",
       "       -0.10197126, -0.40916362, -0.54794914, -0.25149637,  0.02521105,\n",
       "        0.43871549, -0.21258375, -0.14565702, -0.01237976,  0.15953365,\n",
       "       -0.11303504, -0.30115831, -0.15162545,  0.19823261, -0.23235978,\n",
       "       -0.18817002,  0.00194762, -0.26475126,  0.4998897 ,  0.04780847,\n",
       "       -0.67797011, -0.05949406,  0.10687623,  0.16832891, -0.09028279,\n",
       "       -0.10813599, -0.21848199,  0.29010838,  0.31277871, -0.05764629,\n",
       "        0.16654769, -0.09317303,  0.49968228,  0.24927135,  0.42279088,\n",
       "        0.25854558,  0.32184869,  0.00919302, -0.16999032,  0.13100877,\n",
       "       -0.55412215,  0.4105022 ,  0.24070907, -0.18511914,  0.27300489,\n",
       "        0.14057405, -0.17461459, -0.27936918, -0.07487534,  0.17711158,\n",
       "        0.35767546,  0.17930624, -0.29731178,  0.14865193,  0.30058551,\n",
       "       -0.04634576,  0.01784888,  0.0844516 , -0.00594203,  0.1615085 ,\n",
       "       -0.29563186, -0.08857595, -0.31892598, -0.09469842, -0.9708755 ,\n",
       "       -0.34497714,  0.14658549, -0.35853729,  0.05735499, -0.2347174 ,\n",
       "       -0.39321318, -0.03005202,  0.28850845, -0.21731159, -0.08507252,\n",
       "        0.2035646 , -0.16396531, -0.03121888, -0.3250801 ,  0.42348084], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['rey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['rey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22192"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = model.index2word\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'de', u'la', u'el', u'que', u'en', u'y', u'a', u'los', u'del', u'se']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Espacio de representación de palabras\n",
    "\n",
    "Una vez que tenemos la representación de palabras podemos usarlas en otros modelos, o aplicarlos en distintas tareas. En la librería gensim hay tres buenos ejemplo: palabras similares (usando distancia euclideana en espacio de representaciones), analogías y encontrar la palabra distinta de un grupo.\n",
    "\n",
    "### Palabras similares\n",
    "\n",
    "Utiliza distancia euclideana en el espacio de representación distribuida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'tormenta', 0.8314812183380127),\n",
       " (u'humedad', 0.8150405883789062),\n",
       " (u'fr\\xedo', 0.8086050152778625),\n",
       " (u'precipitaci\\xf3n', 0.7927411794662476)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('lluvia', topn = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'saque', 0.8230658173561096),\n",
       " (u'penalti', 0.8180238604545593),\n",
       " (u'remate', 0.8097467422485352),\n",
       " (u'set', 0.8084077835083008),\n",
       " (u'break', 0.807267963886261),\n",
       " (u'empate', 0.8063345551490784),\n",
       " (u'cabezazo', 0.7945455312728882),\n",
       " (u'c\\xf3rner', 0.7887912392616272),\n",
       " (u'minuto', 0.7871119379997253),\n",
       " (u'contragolpe', 0.7827543616294861)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('gol', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'emocionante', 0.9121171832084656),\n",
       " (u'divertido', 0.8937031626701355),\n",
       " (u'maravilloso', 0.8936442732810974),\n",
       " (u'doloroso', 0.879575252532959),\n",
       " (u'arriesgado', 0.877044141292572),\n",
       " (u'inc\\xf3modo', 0.8764861226081848),\n",
       " (u'c\\xf3modo', 0.873559296131134),\n",
       " (u'vistoso', 0.8731124401092529),\n",
       " (u'exigente', 0.8686376810073853),\n",
       " (u'agradable', 0.8677032589912415)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('bonito', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'gracioso', 0.9655297994613647),\n",
       " (u'estupendo', 0.9500259160995483),\n",
       " (u'arriesgado', 0.9461573958396912),\n",
       " (u't\\xedmido', 0.9422381520271301),\n",
       " (u'exagerado', 0.9421201944351196),\n",
       " (u'afortunado', 0.9383031129837036),\n",
       " (u'aburrido', 0.9380552172660828),\n",
       " (u'raro', 0.9332619309425354),\n",
       " (u'tonto', 0.9319804310798645),\n",
       " (u'guapo', 0.9303505420684814)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('feo', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogías: respuesta de preguntas\n",
    "\n",
    "Los vectores de esta representación contienen información sorprendente, tanto de sintaxis como de significado. Por ejemplo, podemos intentar contestar preguntas como:\n",
    "\n",
    "*hombre es a rey como mujer es a ...*. \n",
    "\n",
    "\n",
    "La idea es que en el espacio de representaciones, podemos hacer aritmética básica de conceptos: buscamos encontrar una palabra x tal que los dos vectores v(rey) - v(hombre) y v(mujer) - v(x) sean muy similares.\n",
    "\n",
    "![Imagen](vspace_ops.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'abuela', 0.8424128293991089),\n",
       " (u'hermana', 0.8109989166259766),\n",
       " (u'madre', 0.7994287610054016),\n",
       " (u'novia', 0.7889715433120728),\n",
       " (u'amiga', 0.7852054238319397)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['mujer',u'abuelo'], negative=['hombre'], topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'mujeres', 0.6479780673980713),\n",
       " (u'varones', 0.6390057802200317),\n",
       " (u'ancianos', 0.6382261514663696),\n",
       " (u'maltratadores', 0.6336269974708557),\n",
       " (u'homosexuales', 0.6328884363174438)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['mujer','hombres'], negative=['hombre'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'invitaci\\xf3n', 0.6186299324035645),\n",
       " (u'boda', 0.6086856126785278),\n",
       " (u'princesa', 0.606860339641571),\n",
       " (u'infanta', 0.6054365038871765),\n",
       " (u'viuda', 0.6032512187957764),\n",
       " (u'homil\\xeda', 0.6028061509132385),\n",
       " (u'resurrecci\\xf3n', 0.6002622842788696),\n",
       " (u'comuni\\xf3n', 0.598954975605011),\n",
       " (u'tumba', 0.5981884002685547),\n",
       " (u'majestad', 0.5978625416755676),\n",
       " (u'bendici\\xf3n', 0.5952019095420837),\n",
       " (u'oraci\\xf3n', 0.5945830941200256),\n",
       " (u'reina', 0.5942752957344055),\n",
       " (u'borb\\xf3n', 0.5942150354385376),\n",
       " (u'confederaciones', 0.5939369797706604)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['mujer','rey'], negative=['hombre'], topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'invierno', 0.6695204973220825),\n",
       " (u'oto\\xf1o', 0.6542546153068542),\n",
       " (u'agosto', 0.6259781718254089),\n",
       " (u'enero', 0.6249174475669861),\n",
       " (u'septiembre', 0.6161805987358093)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['nieve','verano'], negative=['arena'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'par\\xeds', 0.7726121544837952),\n",
       " (u'berl\\xedn', 0.7514835596084595),\n",
       " (u'mosc\\xfa', 0.7252907752990723),\n",
       " (u'tokio', 0.7204474210739136),\n",
       " (u'estocolmo', 0.7150971293449402)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['francia','londres'], negative=['inglaterra'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'meses', 0.6087338924407959),\n",
       " (u'semanas', 0.5610102415084839),\n",
       " (u'a\\xf1os', 0.4978870451450348),\n",
       " (u'd\\xe9cada', 0.492570698261261),\n",
       " (u'ejercicios', 0.4863927662372589),\n",
       " (u'lustros', 0.4832686483860016),\n",
       " (u'mandatos', 0.47152382135391235),\n",
       " (u'encuentros', 0.47019052505493164),\n",
       " (u'trimestres', 0.4672251045703888),\n",
       " (u'siglos', 0.4663507044315338),\n",
       " (u'primavera', 0.460256427526474),\n",
       " (u'veranos', 0.45867007970809937),\n",
       " (u'pretemporada', 0.45202308893203735),\n",
       " (u'instantes', 0.44825083017349243),\n",
       " (u'arrestos', 0.4478601813316345),\n",
       " (u'preparativos', 0.44780829548835754),\n",
       " (u'legislatura', 0.4448522627353668),\n",
       " (u'temporadas', 0.44212284684181213),\n",
       " (u'periodos', 0.4336188733577728),\n",
       " (u'quince', 0.4292360246181488)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[u'días','semana'], negative=[u'día'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'playa', 0.59772127866745),\n",
       " (u'sombra', 0.569498598575592),\n",
       " (u'cola', 0.5649012327194214),\n",
       " (u'desembocadura', 0.5639711618423462),\n",
       " (u'orilla', 0.5566952228546143)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['sol','noche'], negative=[u'día'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'dormir', 0.692653477191925),\n",
       " (u'pasear', 0.6907150149345398),\n",
       " (u'beber', 0.687917172908783),\n",
       " (u'cenar', 0.6622120141983032),\n",
       " (u'cocinar', 0.650323212146759)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[u'comer',u'noche'], negative=[u'día'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabra que no pertenece\n",
    "\n",
    "Si tenemos una colección de palabras \"perro gato cuchara\", ¿cuál es la que no pertenece? Podemos usar los vectores. En este caso, calculamos el centroide de los vectores de las palabras proporcionadas, calculamos el centroide, y escogemos la palabra más lejana del centroide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'perro'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(u'comida cena desayuno perro'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presidente'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('gol juego presidente remate portero'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comida'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('francia inglaterra rusia comida italia'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maría\n"
     ]
    }
   ],
   "source": [
    "print model.doesnt_match(u'juan maría pedro fernando josé alberto'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'francia'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('uno dos tres francia cuatro cinco seis siete ocho'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'invierno'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('invierno verano otoño cuchara'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar los vectores de palabras\n",
    "\n",
    "Podemos hacer más cosas extrayendo la matriz de vectores (por ejemplo, usar como entradas para otra tarea de predicción): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22192, 150)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "- [Word2vec en ejercicio de Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "\n",
    "- [word2vec en Spotify, aplicando a playlists  junto con factorización de matrices (34)](http://www.slideshare.net/AndySloane/machine-learning-spotify-madison-big-data-meetup)\n",
    "\n",
    "- [Código original y algunas explicaciones](https://code.google.com/p/word2vec/)\n",
    "\n",
    "- [Paper original con varios resultados](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "- [Presentación](http://www.coling-2014.org/COLING%202014%20Tutorial-fix%20-%20Tomas%20Mikolov.pdf)\n",
    "\n",
    "- [Otra explicación corta](http://arxiv.org/pdf/1402.3722v1.pdf)\n",
    "\n",
    "- [Documentación de word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
