---
title: "Ejemplo de clasificación: k-vmc y mínimos cuadrados"
author: "Felipe González"
date: Otoño 2015
output: 
  html_document: 
    theme: united
---

### Datos

```{r, message=FALSE}
library(kknn)
library(ggplot2)
library(dplyr)
library(tidyr)
options(digits=2)
```



Primero simulamos los datos que queremos predecir. En este ejemplo, la $Y$ 
toma valores 0 o 1, así que podemos interpretar como un problema de clasificación. 
Los datos son simulados como sigue: 

- los datos de clase Y=1 son una mezcla de 10 normales fijas (con distintos centros), y 
- los datos de clase Y=0 son una mezcla de otras 10 normales fijas (con distintos centros). 

En cada caso, los centros de tales normales se generaron también con distribuciones normales.

Primero simulamos los centros y definimos una función para simular datos:

```{r }
set.seed(28057)
centros_negro <- data.frame(x.1=rnorm(10,0,1.5), x.2=rnorm(10,1,1.5))
centros_rojo <- data.frame(x.1=rnorm(10,1,1.5), x.2=rnorm(10,0,1.5))

simular <- function(n, centros_negro, centros_rojo){
  sim_datos <- lapply(1:n, function(i){
  	u <- runif(1)
  	if (u > 0.5){
  		centro <- centros_rojo[sample(1:10,1),]
  		color <- 0
  	} else {
  		centro <- centros_negro[sample(1:10,1), ]		
  		color <- 1
  	}
  	data.frame(color = color, 
  		x.1 = rnorm(1, centro[1,1], 1/2), 
  		x.2 = rnorm(1, centro[1,2], 1/2))
  	})
  rbind_all(sim_datos)
  }
```

Ahora simulamos una muestra de entrenamiento y una muestra grande de prueba:

```{r}
sim_entrena <- simular(500, centros_negro, centros_rojo)
sim_prueba <- simular(3000, centros_negro, centros_rojo)

head(sim_entrena, 10)
ggplot(sim_entrena, aes(x=x.1, y=x.2, colour=factor(color))) + geom_point()
```

### Modelo lineal

En primer lugar ajustamos un modelo de regresión (si sabes regresión logística que veremos más adelante
puedes usarla aquí):

```{r}
modelo_lineal <- lm(color ~ x.1 + x.2, data = sim_entrena)
modelo_lineal
```

Recordemos que cuando la predicción $\hat{y}>=0.5$ clasificamos al color 1, y clasificamos
a color 0 en otro caso. Podemos dibujar entonces la función que clasifica junto con los datos de la siguiente forma:

```{r}
grid_1 <- expand.grid(x.1 = seq(-6,5, 0.05), x.2 = seq(-6,5,0.05))

graf_clasificador <- function(graf_out){
  
  graf_out$fitted[graf_out$fitted > 0.499 & graf_out$fitted < 0.501] <- NA
  plot_out <- ggplot(graf_out, aes(x=x.1,y=x.2)) + 
	    geom_point(alpha=0.9, size=2, aes(colour = as.numeric(fitted>0.5))) + 
      scale_colour_gradient(low = '#F8766D', high='#00BFC4', na.value='gray20') +
	    geom_point(aes(x=x.1, y=x.2, fill = factor(color)), alpha=0.6, 
	           pch=21, colour='gray20', data=sim_entrena, size=3)
  plot_out
}

```

```{r}
graf_out <- grid_1
graf_out$fitted <- predict(modelo_lineal, newdata = grid_1) 
print(graf_clasificador(graf_out))
```

¿Por qué la frontera de clasificación entre las dos clases es una recta? La respuesta
es que que la regla $\hat{y}=0.5$ es un hiperplano, pues del lado izquierdo hay una función
lineal de $x.1$ y $x.2$.

Ahora calculamos tasas de clasificación incorrecta de entrenamiento y prueba:

```{r}
reg_entrena <- mean(sim_entrena$color != as.numeric(fitted(modelo_lineal)>0.5))
preds <- predict(modelo_lineal, newdata = sim_prueba)
reg_prueba <- mean(sim_prueba$color != as.numeric(preds > 0.5))
reg_entrena
reg_prueba
```

### k-vecinos más cercanos

Ahora probamos con k-vecinos más cercanos. Nótese que las regiones de clasificación son mucho más
flexibles que en regresión, y obtenemos una regla de clasificación más compleja:

```{r}
vmc <- kknn(color ~ x.1 + x.2, 
	train = sim_entrena, test = grid_1, k = 20, kernel='rectangular') # encontrar vecinos más cercanos en grid
graf_out$fitted <- predict(vmc) 
print(graf_clasificador(graf_out))
```


Si usamos menos vecinos, entonces el clasificador es aún más complicado:


```{r}
vmc <- kknn(color ~ x.1 + x.2, 
	train = sim_entrena, test = grid_1, k = 2, kernel='rectangular') # encontrar vecinos más cercanos en grid
graf_out$fitted <- predict(vmc) 
print(graf_clasificador(graf_out))
```

Podemos calcular, por ejemplo, errores para 20 vecinos más cercanos:


```{r}
vmc_ent <- kknn(color ~ x.1 + x.2, 
	train = sim_entrena, test = sim_entrena, k = 10, kernel='rectangular') 
fitted <- predict(vmc_ent)
mean(sim_entrena$color != as.numeric(fitted > 0.5))

```

```{r}
vmc_prueba <- kknn(color ~ x.1 + x.2, 
	train = sim_entrena, test = sim_prueba, k = 10, kernel='rectangular') 
preds <- predict(vmc_prueba) 
mean(sim_prueba$color != as.numeric(preds > 0.5))
```

- Discute los errores de predicción que acabas de obtener. ¿Cuál parece ser el mejor
clasificador hasta ahora?


### Afinando el parámetro de k-vmc

Podemos investigar más el número de vecinos más apropiado para estos datos:



```{r}
num_vecinos <- data.frame(k = c(1,2,5,7,10,15,20,50,100,150,250,490)) %>% group_by(k)

calcular_errores <- function(dat){
  vecino_k_ent <- kknn(color ~ x.1 + x.2, 
      train = sim_entrena, test = sim_entrena,
      k=dat$k, kernel='rectangular')
  vecino_k_prueba <- kknn(color ~ x.1 + x.2, 
      train = sim_entrena, test = sim_prueba, 
      k=dat$k, kernel='rectangular')
  fit_entrena <- predict(vecino_k_ent) > 0.5
  fit_prueba <- predict(vecino_k_prueba) > 0.5
  prueba <- mean(fit_prueba != sim_prueba$color)
	entrena <- mean( fit_entrena != sim_entrena$color)
	data.frame(k = dat$k, prueba = prueba, entrena = entrena)
}

errores_vmc <- do(num_vecinos, calcular_errores(.)) %>%
  gather(tipo, error, prueba:entrena)
data.frame(errores_vmc)
ggplot(errores_vmc, aes(x = factor(floor(500/k)), y=error, colour=tipo, group=tipo)) + 
  geom_point(size=3) + geom_line() + xlab('Grados de libertad')
```

- En esta última gráfica vemos que una solución de 10-33 grados de libertad (que corresponde
 a unos 15-50 vecinos) es la que da mejor error de prueba (alrededor de 25\%). 
- Esta tasa supera a la del modelo lineal (que era alrededor del 25%) 
 - Nótese también
 que el error de entrenamiento para 1 vecino más cercano es de 0. ¿Por qué?
 - ¿Crees que se puede obtener una tasa de error de predicción igual o muy cercana a 0 para este problema? ¿Por qué?
