---
title: "Introducción - ejemplos"
author: "Felipe González"
date: Otoño 2015
output: 
  html_document: 
    theme: united
---


### Reconocimiento de dígitos: buscando patrones

```{r, message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(ElemStatLearn)
graficar_digitos <- function(datos){
  mat_digitos <- lapply(1:nrow(datos), 
	                         function(x){ 
                            	t(matrix(as.numeric(datos[x, 2:257]), 
                            	         16, 16, byrow = T))[,16:1]
                             })
	image (z = Reduce("rbind", mat_digitos), col = terrain.colors(30))
	text(seq(0,1,1/10) + 0.05, 0.05, label = datos[, 1], cex = 1.5)
}

```

Consideramos imágenes escaneadas de dígitos escritos a mano, procesadas
a 16x16 pixeles.

```{r, fig.width=10, fig.height=3}
zip_train <- data.frame(zip.train)
muestra <- zip_train %>% sample_n(10)
graficar_digitos(muestra)
```

```{r, fig.width=10, fig.height=3}
muestra <- zip_train %>% sample_n(10)
graficar_digitos(muestra)
```

Los 16x16=256 están escritos acomodando las filas de la imagen en 
vector de 256 valores (cada renglón de `zip.train`):

```{r}
dim(zip_train)
#un renglón
x <- as.numeric(zip_train[3,2:257])
x
zip_train[3,1]
```


¿Podemos reconocer los distintos patrones de los dígitos de manera
automática? Nos gustaría poder "ver" en el espacio de dimensión 256 y reconocer
los distintos grupos que forman los distintos dígitos, si es que existe. Podemos
intentar mapear a un espacio de dimensión baja (2), intentando preservar la estructura
local de los datos. En este caso intentamos con [t-sne](https://www.youtube.com/watch?v=RJVL80Gg3lA&list=UUtXKDgv1AVoG88PLl8nGXmw),
que está principalmente en representar estructura local en el espacio de dimensión alta.



```{r}
library(tsne)
set.seed(288022)
muestra_dig <- zip_train %>% sample_n(500)
tsne_digitos <- tsne(muestra_dig[,2:257], perplexity = 30, max_iter = 500, initial_dims = 15)
dat_tsne <- data.frame(tsne_digitos)
dat_tsne$digito <- as.character(muestra_dig$X1)
ggplot(dat_tsne, aes(x=X1, y=X2, colour=digito, label=digito)) + geom_text()
```

En este ejemplo, nunca utilizamos la etiqueta de dígito para cada imagen. Utilizamos una
técnica para representar los datos en dimensión baja, y **descubrimos que por la mayor parte
podemos distinguir los distintos patrones** asociados a distintos dígitos.


### Clasificación de spam: reglas de decisión, predicción

Supongamos que queremos separar emails de spam de los que no lo son.
Una primera idea es que esto podemos decidirlo adecuadamente considerando
las palabras que contienen los correos (por ejemplo si contiene la palabra free o no,
etc.).

Consideramos unos datos que ya han sido preprocesados. Se procesa mail por mail: se separan
en palabras (tokens), se cuentan las palabras distintas, y luego seleccionamos aquellas
que ocurren en el conteo total un mínimo de veces.

Por ejemplo, los datos de [spambase](https://archive.ics.uci.edu/ml/datasets/Spambase) ya están
preprocesados. 

Cada línea representa un correo:

```{r}
spam_train <- read.csv(file = '../../datos/spam/spam_train.csv')
dim(spam_train)
names(spam_train) <- c("wfmake", "wfaddress", "wfall", "wf3d", "wfour",
  "wfover", "wfremove", "wfinternet", "wforder", "wfmail", 
  "wfreceive", "wfwill", "wfpeople", "wfreport", "wfaddresses", 
	"wffree", "wfbusiness", "wfemail", "wfyou", "wfcredit", "wfyour", 
	"wffont", "wf000", "wfmoney", "wfhp", "wfhpl", "wfgeorge", "wf650", 
	"wflab", "wflabs", "wftelnet", "wf857", "wfdata", "wf415", "wf85", 
	"wftechnology", "wf1999", "wfparts", "wfpm", "wfdirect", "wfcs", 
	"wfmeeting", "wforiginal", "wfproject", "wfre", "wfedu", "wftable", 
	"wfconference", "cfsc", "cfpar", "cfbrack", "cfexc", "cfdollar", 
	"cfpound", "crlaverage", "crllongest", "crltotal", "spam")
head(spam_train)
```

Las frecuencias de ocurrencia están divididas entre el número total de palabras
y multiplicadas por 100 (son porcentajes). La última columna indica si el correo es spam o no: 

```{r}
spam_train %>% group_by(spam) %>% tally
```
Frecuencias para las primeras 35 características, promediadas sobre
los spams y los no spams:

```{r}
frec_terminos <- spam_train %>% 
  gather(termino, frec, wfmake:crltotal) %>%
  group_by(spam, termino) %>%
  summarise(media_frec = mean(frec))
indices <- frec_terminos %>% 
  group_by(termino) %>%
  mutate(media_gral = mean(media_frec)) %>%
  mutate(indice = media_frec/media_gral) %>%
  filter(spam == 1) %>%
  select(termino, indice)
frec_terminos <- frec_terminos %>%
  left_join(indices)
```

```{r, fig.height=8}
ggplot(frec_terminos, aes(x = reorder(termino,indice), y = media_frec, colour=factor(spam))) + 
    geom_point() + coord_flip() +
    ylab("Promedio de frecuencia de ocurrencia") + scale_y_log10() 
```

Ordenamos las palabras según la diferencia de frecuencia entre spams y no spams.


¿Qué reglas de clasificación podemos usar? Podemos considerar primero una
regla simple, donde nos fijamos primero si contiene la palabra
"remove":

```{r}
spam_train %>% group_by(wfremove>0, spam) %>% tally %>%
  group_by(`wfremove > 0`) %>% mutate(prop = round(n/sum(n),2))
```

Aquí vemos que entro de el grupo que contiene la palabra "remove"", la proporción
de mail de spam es mucho más alta que en el grupo que no contiene. Nuestra primera regla
podría ser entonces: si contiene la palabra "remove"", probablemente es spam. Sin embargo,
si no la contiene, entonces no está tan claro. Para averiguar esto tenemos
que concentrarnos en el grupo de correos que no contiene la palabra "remove". Por ejemplo,
¿será que las veces que ocurre remove nos ayuda? Parece que no nos va ayudar:


```{r}
ggplot(filter(spam_train, wfremove==0),
       aes(x=wfremove, group=spam, colour=as.character(spam))) + geom_density(adjust = 0.5)
```

¿Qué tal la frecuencia de ocurrencia de la palabra george?

```{r}
ggplot(filter(spam_train, wfremove==0),
       aes(x=0.001+ wfgeorge, group=spam, fill=as.character(spam))) + geom_histogram() +
  scale_x_log10()
```

Vemos que si cortamos

```{r}
grupo <- filter(spam_train, wfremove == 0)
grupo %>% group_by(wfgeorge>1, spam) %>% tally %>%
  group_by(`wfgeorge > 1`) %>% mutate(prop = round(n/sum(n),2))
```

Ahora seguimos buscando, pero en el grupo
```{r}
grupo_nuevo <- filter(spam_train, wfremove == 0, wfgeorge <= 1)
nrow(grupo_nuevo)

```

Podríamos por ejemplo intentar:
```{r}
grupo_nuevo %>% group_by(cfdollar > 0.1, spam) %>% tally %>%
  group_by(`cfdollar > 0.1`) %>% mutate(prop = round(n/sum(n),2))
```

En resumen, hemos logrado algo como

```{r}
library(DiagrammeR)
grViz(" digraph {
  graph [overlap = true]
  node [shape = box, fontname = Helvetica, color = blue]
  edge [color = green]
  A [label ='wfremove > 0 \n spam 39%']
  B [label ='wfgeorge > 1 \n spam 28%']
  C [label = 'cfdollar > 0.1 \n spam 32%']
  node [shape = oval,fixedsize = true,width = 1.5]
  T1 [label = 'spam 95%']
  T2 [label = 'no spam 100%']
  T3 [label = 'spam 87%']
  T4 [label = 'no spam 79%']

  A -> T1
  A -> B [color = red]
  B -> C [color = red]
  B -> T2; C -> T3; C -> T4 [color = red]
}
")
```


Este es un conjunto relativamente simple de reglas recursivas. En cada nodo terminal
simplemente clasificamos según la regla de la mayoría: si hay más spam, clasificamos en spam, y viceversa.

Hay varias preguntas que podemos hacer aquí:

#### ¿Qué tan bueno es nuestro algoritmo?

Nótese que según el árbol de arriba, clasificamos correctamente al menos al 87% de
todos los casos que teníamos. Para calcular la tasa exacta de clasificados correctamente, tendríamos
que ponderar la tasa de correctos de cada nodo con 

Sin embargo, **¿cómo podemos estar seguros de que el algoritmo se va a desempeñar
de manera similar para emails que no hemos visto?** Es decir, ¿cómo entender si este
algoritmo se generaliza?

Lo que puede estar pasando es que estamos escogiendo reglas ad-hoc adaptadas a los
correos que tenemos a la mano. Es decir, **aprendiendo de más** de los datos de entrenamiento.

#### ¿Estamos escogiendo las mejores variables y los mejores cortes?

Por otro lado, quizá deberíamos tener más reglas para hacer un mejor algoritmo. ¿Cómo podemos
automatizar esta selección de reglas? Claramente no es posible ver todos los árboles posibles, pues
contando variables y puntos de corte, el problema combinatorio es astronómico. 

Pero con tres reglas como las de este ejemplo, seguro estamos tirando información útil, es decir, estamos *aprendiendo demasiado poco*

¿Cómo encontramos el balance correcto? No dejar información útil sin utilizar, pero tampoco utilizar
particularidades de nuestros datos que no generalizan en el futuro?

