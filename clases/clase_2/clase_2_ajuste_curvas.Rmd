---
title: "Ajuste de curvas y predicción"
author: "Felipe Gonzalez"
output: 
  html_document: 
    theme: united
---



```{r setup, include = FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
options(digits=2)
```

### Datos

Simulamos datos a partir de un modelo simple como sigue (del modelo "real" $Y=f(X)+\epsilon$):
$$Y=f(X) +\epsilon,$$
donde 
$$f(X)=\sin (2\pi X)+\cos(2\pi X),$$
y $\epsilon \sim N(0,\sigma^2)$, y finalmente $X\sim U(0,1)$. 

Para generar datos de este modelo, primero generamos $x_i\sim U(0,1)$, calculamos $f(x_i)$, y finalmente
sumamos $\epsilon_i \sim N(0,\sigma^2)$ para obtener $y_i$. Con esto tenemos una
muestra de entrenamiento
$$\mathcal{L} =\{ (x_i,y_i), i=1,2,\ldots, n\} $$


```{r }
f <- function(x){
  sin(2*pi*x) + cos(2*pi*x)
}
simular  <- function(n_muestra, sd){
  x <- runif(n_muestra, 0, 1) 
  y <- f(x) + rnorm(length(x), mean = 0, sd = sd_mod)
  data.frame(x, y)
}
set.seed(28) 
sd_mod <- 0.5
datos <- simular(20, sd_mod)
datos
```

Graficamos la función $f(x)$ para el modelo *real* y los datos de entrenamiento:

```{r }
x_plot <- seq(0,1,0.01)
y_plot <- f(x_plot)
ggplot(datos, aes(x=x, y=y), colour='red')+
  geom_point() +
  annotate("line", x=x_plot, y=y_plot, linetype="dotted")
```

### Ajuste o entrenamiento

En este ejemplo, intentaremos ajustar modelos polinomiales a estos datos. Es decir, nuestra
familia está dada por polinomios de la forma
$$g(x)=\sum_{i=0}^p \alpha_j x^j,$$
que ajustamos por mínimos cuadrados resolviendo
$$\min \frac{1}{n}\sum_{i=1}^n \left(y_i - \sum_{j=0}^p \alpha_j x_i^j \right)^2,$$
que nos da coeficientes $\hat{\alpha_0}, \hat{\alpha_1},\ldots, \hat{\alpha_p}$. De 
esta forma escribimos:
$$\hat{f}_{\mathcal L} (x) = \sum_{j=0}^p \hat{\alpha_j}x^j,$$
que es nuestra función de predicción.

Por ejemplo,
supongamos que escogemos trabajar con polinomios de grado $p=6$:

```{r}
ajuste_mod <- function(datos, m){
  lm(y ~ poly(x, degree=m, raw = TRUE), data = datos) 
}
```

Las $\hat{\alpha_j}$ están dadas por:

```{r}
mod <- ajuste_mod(datos, m = 6)
data.frame(coef = coef(mod))
```

Y podemos graficar los datos de entrenamiento, la verdadera $f$ y la $\hat{f}_{\mathcal L} (x)$ ajustada:

```{r}
dat <- data.frame( x=x_plot, prediccion = predict(mod, newdata = data.frame(x=x_plot)),
        esperado = y_plot)

datos_graf <- dat %>% gather(tipo, valor, prediccion:esperado)
ggplot(datos_graf, aes(x=x, y=valor, linetype=tipo )) + 
    geom_line() +
    ylim(c(-3,3)) +
    annotate("point",x=datos$x, y=datos$y, colour="red")
```


### Error de entrenamiento y de prueba

¿Cuál es el error de entrenamiento? Simplemente calculamos
```{r}
ajustados <- predict(mod, newdata = datos)
mean((datos$y - ajustados)^2)
```

¿Cómo estimamos el error de predicción? Podemos simular una muestra grande para probar:
```{r}
datos_prueba <- simular(1000, sd)
predicciones <- predict(mod, newdata = datos_prueba)
mean((datos_prueba$y - predicciones)^2)
```

Nótese que el error de entrenamiento es bastante más chico que el error de predicción.

- ¿Cuáles son los defectos que puedes ver en el modelo que ajustamos? ¿Por qué ocurren estos defectos?
- ¿Es natural esperar que el error de entrenamiento tienda a ser menor que el de predicción?




### Modelos de distinta complejidad

Ahora veremos cómo se ve este problema cuando ajustamos polinomios de varios grados. Por ejemplo: 


```{r }
modelos <- lapply(1:9, function(i) ajuste_mod(datos, i))
modelos[[1]]
modelos[[3]]
modelos[[7]]
```

Graficaremos para ver qué modelos ajustamos y cómo van a ser sus predicciones. Nótese que tanto el modelo lineal como el modelo de grado 9 ajustan mal, pero por diferentes razones:

- ¿Por qué ajustan mal los modelos de grado demasiado bajo?
- ¿Por qué ajustan mal los modelos de grado muy alto?

```{r }
datos_graf_lista <- lapply(1:9, function(i){
   df <- data.frame(grado = i,  x=x_plot , 
        prediccion = predict(modelos[[i]], newdata = data.frame(x=x_plot)),
        esperado = y_plot)
    df   
})
datos_graf <- rbind_all(datos_graf_lista)

dat <- datos_graf %>% gather(variable, valor, prediccion:esperado)
ggplot(dat, aes(x=x, y=valor, linetype=variable )) + 
    geom_line() +
    facet_wrap(~grado) + 
    ylim(c(-3,3)) + 
    annotate("point",x=datos$x, y=datos$y, colour="black")
```



### Error de entrenamiento y error de prueba 

Calculamos los errores de entrenamiento:

```{r }
errores.entrenamiento <- sapply(modelos, function(mod){ 
    ajustados.entrenamiento <- fitted(mod)
    (mean( (datos$y - ajustados.entrenamiento)^2))
    })
errores.entrenamiento
```

Y ahora estimamos el error de predicción con una muestra de prueba de tamaño 500:

```{r }
xp <- runif(1000)
yp <-  sin(2*pi*xp) + cos(2*pi*xp) +  rnorm(length(xp), mean = 0, sd = sd_mod)
errores.prueba <- sapply(modelos, function(mod){
    ajustados.prueba <- predict(mod, newdata = data.frame(x=xp))
    (mean( (yp - ajustados.prueba )^2))
    })
errores.prueba
```


Y comparamos los dos errores:

```{r }
errores <- data.frame(grado=1:9, entrenamiento=errores.entrenamiento, 
    prueba = errores.prueba)
errores.m <- errores %>% gather(var, valor, entrenamiento:prueba)
graf.3 <- ggplot(errores.m, aes(x=grado, y=valor, linetype=var)) + geom_point()+
    geom_line() + ylab("Error") 
graf.3
```

En esta última gráfica vemos un patrón que resultará ser usual:


- Modelos demasiado rígidos (en este caso de grado bajo) no capturan señal en los datos y por lo tanto son malos en la predicción, mientras que 
- Modelos demasiado flexibles (grado alto en este caso) sobreajustan fácilmente los datos, y también son malos en la predicción. 

- Modelos más complejos siempre ajustan mejor a los datos de **entrenamiento** que modelos más simples (el error de entrenamiento siempre disminuye cuando aumentamos complejidad).

- Pero modelos más complejos no necesariamente son mejores en la predicción.

- Estas dos curvas dependen de las particularidades de los datos que usamos para ajustar o entrenar los polinomios, pero las afirmaciones de arriba siempre se cumplen.



### Algunas conclusiones


- <span style="color:purple"> **El mejor modelo no es el que reduce más el error de entrenamiento** </span>

- <span style="color:purple"> **El error de entrenamiento es generalmente un estimador pobre del error de predicción de un modelo**</span>

- <span style="color:purple"> **Los mejores modelos tienen el nivel de complejidad adecuada para el problema, incluyendo el tamaño de muestra**</span>






### Error de entrenamiento y de prueba promediando sobre conjuntos de entrenamiento.

 Como ejercicio adicional, podemos preguntarnos cómo se comportan nuestros modelos ajustados en promedio sobre nuevas muestras generadas por el fenómeno que nos interesa. Abajo repetimos el 
 proceso de arriba 300 veces: producimos  300 muestras de entrenamiento, ajustamos, y estimamos
 el error de predicción con la misma muestra de tamaño 1000:




```{r}
# muestra de prueba fija.

salida.sim.lista <- lapply(1:500, function(i){
    # simular muestra de entrenamiento
    datos <- simular(n_muestra = 30, sd_mod)
    # ajustar polinomios
    modelos <- lapply(1:9, function(m){
            mod <- lm(y ~ poly(x, degree=m, raw = TRUE), data = datos)
        mod
    })
    #calcular error de entrenamiento
    error_entrenamiento <- sapply(modelos, function(mod){ 
        ajustados_entrenamiento <- fitted(mod)
        sqrt(mean( (datos$y - ajustados_entrenamiento)^2))
    })

    # calcular estimación de error de prueba con la muestra de arriba
    error_prueba <- sapply(modelos, function(mod){
        ajustados.prueba <- predict(mod, newdata = data.frame(x=datos_prueba$x))
        sqrt(mean( (datos_prueba$y - ajustados.prueba )^2))
    })
    # regresar en un data.frame la evaluación de los modelos
    data.frame(grado = 1:9, error_entrenamiento, error_prueba, id=i)
})
salida.sim <- rbind_all(salida.sim.lista)

resumen.sim <- salida.sim %>% 
  group_by(grado) %>%
  dplyr::summarise(error_prueba=mean(error_prueba), error_entrenamiento = mean(error_entrenamiento)) %>%
  gather(tipo, error, -grado)
  
ggplot(resumen.sim,
     aes(x=grado, y=error, col=tipo, group=tipo)) + geom_line() +
    geom_point()
```

En esta gráfica observamos claramente el patrón que discutimos arriba:

- Estas últimas gráficas evalúan el método, no un modelo particular. En este sentido son gráficas teóricas.

Podemos ver también estas curvas junto con los resultados para varias muestras de entrenamiento. En la siguiente gráfica, cada línea delgada corresponde a una muestra de entrenamiento distinta:


```{r}
salida_graf <- salida.sim %>% 
  gather(tipo, error, error_entrenamiento:error_prueba)

ggplot(filter(salida_graf, id<=100), aes(x=grado, y=error)) + 
  geom_line(aes(group=interaction(id, tipo), colour=tipo),alpha=0.5) +
  scale_y_log10(breaks=c(0.25,0.5,1,2,4,8,16)) +
  geom_line(data=resumen.sim, aes(group=tipo))
```


